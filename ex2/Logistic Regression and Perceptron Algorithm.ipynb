{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Liabraries\n",
    "import pandas as pd # Dataframe and Data Import Library\n",
    "import numpy as np # Linear Algebra Library\n",
    "\n",
    "# Plotting Library\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy.plotting import plot_implicit\n",
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "\n",
    "# Dataset Lib\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]\n",
      " [ 5.4  3.9  1.7  0.4]\n",
      " [ 4.6  3.4  1.4  0.3]\n",
      " [ 5.   3.4  1.5  0.2]\n",
      " [ 4.4  2.9  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.4  3.7  1.5  0.2]\n",
      " [ 4.8  3.4  1.6  0.2]\n",
      " [ 4.8  3.   1.4  0.1]\n",
      " [ 4.3  3.   1.1  0.1]\n",
      " [ 5.8  4.   1.2  0.2]\n",
      " [ 5.7  4.4  1.5  0.4]\n",
      " [ 5.4  3.9  1.3  0.4]\n",
      " [ 5.1  3.5  1.4  0.3]\n",
      " [ 5.7  3.8  1.7  0.3]\n",
      " [ 5.1  3.8  1.5  0.3]\n",
      " [ 5.4  3.4  1.7  0.2]\n",
      " [ 5.1  3.7  1.5  0.4]\n",
      " [ 4.6  3.6  1.   0.2]\n",
      " [ 5.1  3.3  1.7  0.5]\n",
      " [ 4.8  3.4  1.9  0.2]\n",
      " [ 5.   3.   1.6  0.2]\n",
      " [ 5.   3.4  1.6  0.4]\n",
      " [ 5.2  3.5  1.5  0.2]\n",
      " [ 5.2  3.4  1.4  0.2]\n",
      " [ 4.7  3.2  1.6  0.2]\n",
      " [ 4.8  3.1  1.6  0.2]\n",
      " [ 5.4  3.4  1.5  0.4]\n",
      " [ 5.2  4.1  1.5  0.1]\n",
      " [ 5.5  4.2  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.   3.2  1.2  0.2]\n",
      " [ 5.5  3.5  1.3  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 4.4  3.   1.3  0.2]\n",
      " [ 5.1  3.4  1.5  0.2]\n",
      " [ 5.   3.5  1.3  0.3]\n",
      " [ 4.5  2.3  1.3  0.3]\n",
      " [ 4.4  3.2  1.3  0.2]\n",
      " [ 5.   3.5  1.6  0.6]\n",
      " [ 5.1  3.8  1.9  0.4]\n",
      " [ 4.8  3.   1.4  0.3]\n",
      " [ 5.1  3.8  1.6  0.2]\n",
      " [ 4.6  3.2  1.4  0.2]\n",
      " [ 5.3  3.7  1.5  0.2]\n",
      " [ 5.   3.3  1.4  0.2]\n",
      " [ 7.   3.2  4.7  1.4]\n",
      " [ 6.4  3.2  4.5  1.5]\n",
      " [ 6.9  3.1  4.9  1.5]\n",
      " [ 5.5  2.3  4.   1.3]\n",
      " [ 6.5  2.8  4.6  1.5]\n",
      " [ 5.7  2.8  4.5  1.3]\n",
      " [ 6.3  3.3  4.7  1.6]\n",
      " [ 4.9  2.4  3.3  1. ]\n",
      " [ 6.6  2.9  4.6  1.3]\n",
      " [ 5.2  2.7  3.9  1.4]\n",
      " [ 5.   2.   3.5  1. ]\n",
      " [ 5.9  3.   4.2  1.5]\n",
      " [ 6.   2.2  4.   1. ]\n",
      " [ 6.1  2.9  4.7  1.4]\n",
      " [ 5.6  2.9  3.6  1.3]\n",
      " [ 6.7  3.1  4.4  1.4]\n",
      " [ 5.6  3.   4.5  1.5]\n",
      " [ 5.8  2.7  4.1  1. ]\n",
      " [ 6.2  2.2  4.5  1.5]\n",
      " [ 5.6  2.5  3.9  1.1]\n",
      " [ 5.9  3.2  4.8  1.8]\n",
      " [ 6.1  2.8  4.   1.3]\n",
      " [ 6.3  2.5  4.9  1.5]\n",
      " [ 6.1  2.8  4.7  1.2]\n",
      " [ 6.4  2.9  4.3  1.3]\n",
      " [ 6.6  3.   4.4  1.4]\n",
      " [ 6.8  2.8  4.8  1.4]\n",
      " [ 6.7  3.   5.   1.7]\n",
      " [ 6.   2.9  4.5  1.5]\n",
      " [ 5.7  2.6  3.5  1. ]\n",
      " [ 5.5  2.4  3.8  1.1]\n",
      " [ 5.5  2.4  3.7  1. ]\n",
      " [ 5.8  2.7  3.9  1.2]\n",
      " [ 6.   2.7  5.1  1.6]\n",
      " [ 5.4  3.   4.5  1.5]\n",
      " [ 6.   3.4  4.5  1.6]\n",
      " [ 6.7  3.1  4.7  1.5]\n",
      " [ 6.3  2.3  4.4  1.3]\n",
      " [ 5.6  3.   4.1  1.3]\n",
      " [ 5.5  2.5  4.   1.3]\n",
      " [ 5.5  2.6  4.4  1.2]\n",
      " [ 6.1  3.   4.6  1.4]\n",
      " [ 5.8  2.6  4.   1.2]\n",
      " [ 5.   2.3  3.3  1. ]\n",
      " [ 5.6  2.7  4.2  1.3]\n",
      " [ 5.7  3.   4.2  1.2]\n",
      " [ 5.7  2.9  4.2  1.3]\n",
      " [ 6.2  2.9  4.3  1.3]\n",
      " [ 5.1  2.5  3.   1.1]\n",
      " [ 5.7  2.8  4.1  1.3]\n",
      " [ 6.3  3.3  6.   2.5]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 7.1  3.   5.9  2.1]\n",
      " [ 6.3  2.9  5.6  1.8]\n",
      " [ 6.5  3.   5.8  2.2]\n",
      " [ 7.6  3.   6.6  2.1]\n",
      " [ 4.9  2.5  4.5  1.7]\n",
      " [ 7.3  2.9  6.3  1.8]\n",
      " [ 6.7  2.5  5.8  1.8]\n",
      " [ 7.2  3.6  6.1  2.5]\n",
      " [ 6.5  3.2  5.1  2. ]\n",
      " [ 6.4  2.7  5.3  1.9]\n",
      " [ 6.8  3.   5.5  2.1]\n",
      " [ 5.7  2.5  5.   2. ]\n",
      " [ 5.8  2.8  5.1  2.4]\n",
      " [ 6.4  3.2  5.3  2.3]\n",
      " [ 6.5  3.   5.5  1.8]\n",
      " [ 7.7  3.8  6.7  2.2]\n",
      " [ 7.7  2.6  6.9  2.3]\n",
      " [ 6.   2.2  5.   1.5]\n",
      " [ 6.9  3.2  5.7  2.3]\n",
      " [ 5.6  2.8  4.9  2. ]\n",
      " [ 7.7  2.8  6.7  2. ]\n",
      " [ 6.3  2.7  4.9  1.8]\n",
      " [ 6.7  3.3  5.7  2.1]\n",
      " [ 7.2  3.2  6.   1.8]\n",
      " [ 6.2  2.8  4.8  1.8]\n",
      " [ 6.1  3.   4.9  1.8]\n",
      " [ 6.4  2.8  5.6  2.1]\n",
      " [ 7.2  3.   5.8  1.6]\n",
      " [ 7.4  2.8  6.1  1.9]\n",
      " [ 7.9  3.8  6.4  2. ]\n",
      " [ 6.4  2.8  5.6  2.2]\n",
      " [ 6.3  2.8  5.1  1.5]\n",
      " [ 6.1  2.6  5.6  1.4]\n",
      " [ 7.7  3.   6.1  2.3]\n",
      " [ 6.3  3.4  5.6  2.4]\n",
      " [ 6.4  3.1  5.5  1.8]\n",
      " [ 6.   3.   4.8  1.8]\n",
      " [ 6.9  3.1  5.4  2.1]\n",
      " [ 6.7  3.1  5.6  2.4]\n",
      " [ 6.9  3.1  5.1  2.3]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 6.8  3.2  5.9  2.3]\n",
      " [ 6.7  3.3  5.7  2.5]\n",
      " [ 6.7  3.   5.2  2.3]\n",
      " [ 6.3  2.5  5.   1.9]\n",
      " [ 6.5  3.   5.2  2. ]\n",
      " [ 6.2  3.4  5.4  2.3]\n",
      " [ 5.9  3.   5.1  1.8]]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv('ex2data1.txt')\n",
    "iris = datasets.load_iris()\n",
    "# Feature Scaling\n",
    "x1_Normalizer = max(dataset.iloc[:,0])\n",
    "x2_Normalizer = max(dataset.iloc[:,1])\n",
    "dataset.iloc[:,0] = dataset.iloc[:,0]/x1_Normalizer\n",
    "dataset.iloc[:,1] = dataset.iloc[:,1]/x2_Normalizer\n",
    "print(np.array(iris['data']))\n",
    "# X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def featureMap(x1,x2,degree):\n",
    "    x = np.ones([len(x2), 1])\n",
    "    for i in range(1,degree+1):\n",
    "        for j in range(0,i+1):\n",
    "            x = np.concatenate( (x, ((x1**(i-j))*(x2**j)).reshape(len(x1),1)), axis=1 )\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the X and y trainiing sets\n",
    "X_init = dataset.iloc[:,[0,1]]\n",
    "y_init = dataset.iloc[:,2]\n",
    "def create_dataset(X_train, y_train, degree):\n",
    "#     print('X Data: \\n\\n', X_train.head())\n",
    "#     print('\\n\\n')\n",
    "#     print('y Data: \\n\\n', y_train.head())\n",
    "    \n",
    "    # Convert into numpy arrays\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train).reshape(len(y_train),1)\n",
    "    X_train = featureMap(X_train[:,0],X_train[:,1],degree)\n",
    "\n",
    "    m = len(y_train) # Number of Examples\n",
    "    print('Training Set: ', X_train)\n",
    "    \n",
    "    return [X_train, y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "def plot(X,y,w):\n",
    "    # Seperate Classes\n",
    "    X_postive1 = X[:,1].reshape(99,1)[y == 1].reshape(60,1)\n",
    "    X_postive2 = X[:,2].reshape(99,1)[y == 1].reshape(60,1)\n",
    "    X_postive = np.concatenate( (X_postive1, X_postive2 ), axis =1)\n",
    "\n",
    "    X_negative1 = X[:,1].reshape(99,1)[y == 0].reshape(39,1)\n",
    "    X_negative2 = X[:,2].reshape(99,1)[y == 0].reshape(39,1)\n",
    "    X_negative = np.concatenate( (X_negative1, X_negative2 ), axis =1)\n",
    "    plt.figure\n",
    "    plt.scatter(X_postive[:,0], X_postive[:,1], marker = 'x', color = 'r', EdgeColor = 'r')\n",
    "    plt.scatter(X_negative[:,0], X_negative[:,1], marker = 'o', color='y', EdgeColor = 'b')\n",
    "    plt.xlabel('Exam 1 Scores')\n",
    "    plt.ylabel('Exam 1 Scores')\n",
    "#     if w.any():\n",
    "#         plot_x = [min(X_train[:,1])-0.1,  max(X_train[:,1])+0.1]\n",
    "#         plot_y = (-1./weights[2])*(weights[1]*plot_x + weights[0]);\n",
    "#         plt.plot(plot_x, plot_y)\n",
    "    plt.show()\n",
    "    \n",
    "def ezplot(s):\n",
    "    #Parse doesn't parse = sign so split\n",
    "    lhs, rhs = s.replace(\"^\",\"**\").split(\"=\")\n",
    "    eqn_lhs = parse_expr(lhs)\n",
    "    eqn_rhs = parse_expr(rhs)\n",
    "\n",
    "    plot_implicit(eqn_lhs-eqn_rhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost function with Derivation\n",
    "def sigmoid(deriv, x):\n",
    "    if deriv == True:\n",
    "        return sigmoid(False, x)*(1-sigmoid(False, x))\n",
    "    return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "def perceptron_func(deriv,z):\n",
    "    return np.array(z>=0, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impliment the Cross Entropy Log Loss Function \n",
    "def costFunction(X,y,initW,current_epoch,stochastic=False,mini_batch_size=10, cost_func = 'sigmoid'):\n",
    "    m = len(y)\n",
    "    J = 0\n",
    "    derivatives = np.zeros([X.shape[1],1])\n",
    "    g = np.dot(X,initW)  \n",
    "    \n",
    "    if (cost_func == 'sigmoid'):\n",
    "        hypothesis = sigmoid(False,g)\n",
    "        J = sum (-1*np.dot(y.reshape(m), np.log(hypothesis)) - ( np.dot((1-y.reshape(m)),np.log(1-hypothesis)) )) / m \n",
    "        if not current_epoch%1000:\n",
    "            print('Error at iteration ', current_epoch, ': ', J)\n",
    "            print('Still Learning - Trying to get better...\\n\\n')\n",
    "    elif (cost_func == 'perceptron'):\n",
    "        hypothesis = perceptron_func(False,g)\n",
    "        \n",
    "#     for i in range(0,len(y)):\n",
    "#         newCost = ( -1*y[i]*math.log(hypothesis[i]) ) - ( (1-y[i])*math.log(1-hypothesis[i]) )\n",
    "#         J = J + newCost\n",
    "        \n",
    "#     J = J/m\n",
    "#     print( (hypothesis-y).reshape() )\n",
    "    \n",
    "#     derivatives = np.sum( np.dot( (hypothesis - y).reshape(y.shape[0]), X ).reshape(2,1), axis = 1).reshape(X_train.shape[1],1) / m\n",
    "\n",
    "    if stochastic:\n",
    "        batch_size = 10\n",
    "    else:\n",
    "        batch_size = m\n",
    "\n",
    "    for i in range(0,len(derivatives)):\n",
    "        for j in range(0,batch_size):\n",
    "            diffCost = ( hypothesis[j] - y[j] ) * X[j,i]\n",
    "            derivatives[i] = derivatives[i] + diffCost\n",
    "        derivatives[i] = derivatives[i]/m\n",
    "\n",
    "#     print('Derivatives: ', derivatives)\n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x,weights,cost_func):\n",
    "    X = x # Create a Copy\n",
    "    predictions = []\n",
    "    for i in range(0,len(X)):\n",
    "        x = featureMap(np.array([X[i,0]]),np.array([X[i,1]]),degree)\n",
    "        g = np.dot(x,weights)  \n",
    "        if (cost_func == 'sigmoid'):\n",
    "            prediction = np.round(sigmoid(False,g))[0,0]\n",
    "        elif (cost_func == 'perceptron'):\n",
    "            prediction = np.round(perceptron_func(False,g))[0,0]\n",
    "        predictions.append(prediction)\n",
    "        #print('The Predicted Class for the input is: ', prediction)\n",
    "    return np.array(predictions).reshape(len(predictions),1)\n",
    "\n",
    "def accuracy(X,y,weights,cost_func):\n",
    "    test_vals = np.concatenate((X[:,1].reshape(len(X[:,1]),1), X[:,2].reshape(len(X[:,2]),1)), axis = 1)\n",
    "    predictions = predict(test_vals,weights,cost_func)\n",
    "    print('Training Accuracy: ', np.mean(predictions == y) * 100, ' %')\n",
    "    \n",
    "def train(X, y, initW, num_of_epochs, learning_rate, degree, stochastic=False,mini_batch_size=10, cost_func='sigmoid'):\n",
    "    [X, y] = create_dataset(X, y, degree)\n",
    "    if not len(initW):\n",
    "        # Randomly Initialize the weights\n",
    "        initW = np.random.randn(X.shape[1],1)\n",
    "    weights = initW\n",
    "    print('Initial Weights: ', initW, '\\n\\n')\n",
    "    for i in range(1,num_of_epochs):\n",
    "        deriv = costFunction(X,y,weights,i,stochastic,mini_batch_size, cost_func)\n",
    "        weights = weights - learning_rate*deriv \n",
    "#         print('Weights: ',weights)\n",
    "#     plot(weights)\n",
    "    accuracy(X,y,weights,cost_func)\n",
    "#     predictions = np.dot(X,weights)  \n",
    "#     print('Training Accuracy: ', np.mean(predictions == y) * 100, ' %')\n",
    "    return [X,y,weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "degree = 2 # Needs to be set explicitly globally outside the train function\n",
    "num_of_epochs = 1000\n",
    "learning_rate = 5.5\n",
    "initial_Weights = []\n",
    "\n",
    "# X_init = iris['data']\n",
    "# x_1 = np.ones([len(X_init), 1])\n",
    "# X_init = np.concatenate( (x_1, X_init), axis=1 )\n",
    "# y_init = np.zeros([150,1])\n",
    "# y_init[0:49] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:  [[ 1.          0.30338937  0.44396933  0.09204511  0.13469558  0.19710877]\n",
      " [ 1.          0.35909224  0.73735829  0.12894723  0.26477964  0.54369725]\n",
      " [ 1.          0.60286378  0.87295484  0.36344473  0.52627285  0.76205015]\n",
      " [ 1.          0.79169019  0.76205933  0.62677336  0.6033149   0.58073443]\n",
      " [ 1.          0.45161019  0.56960345  0.20395176  0.25723872  0.32444809]\n",
      " [ 1.          0.61212036  0.97615026  0.37469134  0.59752145  0.95286934]\n",
      " [ 1.          0.75154118  0.47086355  0.56481414  0.35387335  0.22171248]\n",
      " [ 1.          0.76230011  0.88420217  0.58110145  0.67402741  0.78181348]\n",
      " [ 1.          0.84578415  0.44031194  0.71535083  0.37240886  0.19387461]\n",
      " [ 1.          0.96026858  0.38662381  0.92211574  0.3712627   0.14947797]\n",
      " [ 1.          0.75143011  0.30953209  0.56464721  0.23259173  0.09581012]\n",
      " [ 1.          0.82448983  0.77356529  0.67978348  0.63779671  0.59840325]\n",
      " [ 1.          0.694842    0.98836098  0.48280541  0.68675472  0.97685742]\n",
      " [ 1.          0.39606519  0.76906286  0.15686763  0.30459902  0.59145768]\n",
      " [ 1.          0.54064119  0.90227429  0.2922929   0.48780665  0.8140989 ]\n",
      " [ 1.          0.69189248  0.53343553  0.4787152   0.36908003  0.28455346]\n",
      " [ 1.          0.68064022  0.4721234   0.46327111  0.32134618  0.2229005 ]\n",
      " [ 1.          0.70783358  0.93989752  0.50102837  0.66529103  0.88340736]\n",
      " [ 1.          0.77111525  0.48119991  0.59461873  0.37106059  0.23155335]\n",
      " [ 1.          0.67488203  0.43328293  0.45546576  0.29241486  0.18773409]\n",
      " [ 1.          0.89831414  0.66551777  0.80696829  0.59784402  0.4429139 ]\n",
      " [ 1.          0.5062193   0.49414474  0.25625798  0.2501456   0.24417902]\n",
      " [ 1.          0.34271056  0.44715061  0.11745053  0.15324324  0.19994367]\n",
      " [ 1.          0.78058463  0.69761054  0.60931236  0.54454406  0.48666046]\n",
      " [ 1.          0.62378393  0.70754382  0.38910639  0.44135446  0.50061825]\n",
      " [ 1.          0.8032846   0.45334161  0.64526615  0.36416233  0.20551861]\n",
      " [ 1.          0.93274954  0.39244353  0.87002171  0.36605152  0.15401192]\n",
      " [ 1.          0.61936825  0.50830783  0.38361703  0.31482973  0.25837685]\n",
      " [ 1.          0.38852686  0.65738901  0.15095312  0.25541329  0.43216032]\n",
      " [ 1.          0.61485131  0.7364044   0.37804214  0.45277921  0.54229144]\n",
      " [ 1.          0.8555179   0.57704369  0.73191088  0.49367121  0.33297942]\n",
      " [ 1.          0.52197834  0.63849483  0.27246139  0.33328047  0.40767565]\n",
      " [ 1.          0.52135151  0.70226819  0.2718074   0.36612859  0.49318062]\n",
      " [ 1.          0.40306278  0.71981546  0.1624596   0.29013082  0.51813429]\n",
      " [ 1.          0.54729318  0.52810948  0.29952982  0.28903071  0.27889962]\n",
      " [ 1.          0.33973984  1.          0.11542316  0.33973984  1.        ]\n",
      " [ 1.          0.64287655  0.81833238  0.41329026  0.5260867   0.66966788]\n",
      " [ 1.          0.74918219  0.42048804  0.56127395  0.31502215  0.17681019]\n",
      " [ 1.          0.34242586  0.76098058  0.11725547  0.26057943  0.57909144]\n",
      " [ 1.          0.84047074  0.56951924  0.70639107  0.47866426  0.32435217]\n",
      " [ 1.          0.51636609  0.47392088  0.26663394  0.24471667  0.224601  ]\n",
      " [ 1.          0.94606225  0.66318697  0.89503378  0.62741616  0.43981696]\n",
      " [ 1.          0.8251079   0.41082722  0.68080304  0.33897678  0.168779  ]\n",
      " [ 1.          0.51135778  0.4634668   0.26148678  0.23699736  0.21480148]\n",
      " [ 1.          0.62329972  0.52656305  0.38850254  0.3282066   0.27726865]\n",
      " [ 1.          0.77326146  0.71263884  0.59793328  0.55105615  0.50785412]\n",
      " [ 1.          0.97940196  0.87719548  0.95922819  0.85912697  0.76947191]\n",
      " [ 1.          0.62180102  0.97875368  0.38663651  0.60859004  0.95795877]\n",
      " [ 1.          0.91722868  0.89710528  0.84130846  0.82285069  0.80479788]\n",
      " [ 1.          0.80082674  0.75011169  0.64132347  0.6007095   0.56266754]\n",
      " [ 1.          0.99443712  0.6169655   0.98890518  0.61353339  0.38064643]\n",
      " [ 1.          0.90702852  0.4388677   0.82270074  0.39806552  0.19260486]\n",
      " [ 1.          0.34584048  0.6108697   0.11960563  0.21126347  0.37316179]\n",
      " [ 1.          0.5037321   0.5037405   0.25374603  0.25375026  0.25375449]\n",
      " [ 1.          0.49672184  0.60492862  0.24673259  0.30048126  0.36593863]\n",
      " [ 1.          0.97814013  0.69648999  0.95675812  0.68126481  0.48509831]\n",
      " [ 1.          0.32633376  0.9669171   0.10649372  0.31553769  0.93492867]\n",
      " [ 1.          0.74376725  0.7062301   0.55318972  0.52527082  0.49876095]\n",
      " [ 1.          0.71920267  0.79350673  0.51725248  0.57069216  0.62965292]\n",
      " [ 1.          0.75525623  0.86740595  0.57041197  0.65511375  0.75239308]\n",
      " [ 1.          0.3534696   0.4755819   0.12494076  0.16810374  0.22617814]\n",
      " [ 1.          0.56350821  0.39710424  0.3175415   0.2237715   0.15769178]\n",
      " [ 1.          0.30110656  0.50160066  0.09066516  0.15103525  0.25160322]\n",
      " [ 1.          0.44745287  0.67209938  0.20021407  0.3007328   0.45171757]\n",
      " [ 1.          0.66675671  0.41561983  0.44456452  0.27711731  0.17273984]\n",
      " [ 1.          0.40527316  0.98650493  0.16424633  0.39980397  0.97319197]\n",
      " [ 1.          0.49157183  0.52476492  0.24164287  0.25795966  0.27537822]\n",
      " [ 1.          0.80418007  0.93169401  0.64670559  0.74924976  0.86805372]\n",
      " [ 1.          0.66861816  0.61688826  0.44705024  0.41246269  0.38055112]\n",
      " [ 1.          0.3277926   0.43802387  0.10744799  0.14358098  0.19186491]\n",
      " [ 1.          0.64149749  0.78923974  0.41151903  0.50629532  0.62289937]\n",
      " [ 1.          0.72471248  0.97327948  0.52520818  0.70534778  0.94727295]\n",
      " [ 1.          0.60562139  0.73930834  0.36677726  0.44774094  0.54657682]\n",
      " [ 1.          0.58942421  0.76725884  0.3474209   0.45224093  0.58868612]\n",
      " [ 1.          1.          0.73196789  1.          0.73196789  0.53577699]\n",
      " [ 1.          0.47345771  0.8948758   0.2241622   0.42368585  0.80080269]\n",
      " [ 1.          0.50545169  0.76676739  0.25548141  0.38756388  0.58793224]\n",
      " [ 1.          0.60559805  0.4299449   0.366749    0.26037379  0.18485262]\n",
      " [ 1.          0.82368452  0.43208377  0.6784562   0.35590071  0.18669638]\n",
      " [ 1.          0.89067219  0.7060199   0.79329694  0.62883229  0.4984641 ]\n",
      " [ 1.          0.94998038  0.46216818  0.90246273  0.4390507   0.21359942]\n",
      " [ 1.          0.67435342  0.67350797  0.45475254  0.45418241  0.45361299]\n",
      " [ 1.          0.57337408  0.60194823  0.32875784  0.34514151  0.36234167]\n",
      " [ 1.          0.8050534   0.9200027   0.64811097  0.7406513   0.84640497]\n",
      " [ 1.          0.68586588  0.86573071  0.47041201  0.59377516  0.74948967]\n",
      " [ 1.          0.42148009  0.7974637   0.17764547  0.33611507  0.63594835]\n",
      " [ 1.          0.75607855  0.91458537  0.57165477  0.69149838  0.83646639]\n",
      " [ 1.          0.78771023  0.97752583  0.6204874   0.77000709  0.95555675]\n",
      " [ 1.          0.52438272  0.614644    0.27497724  0.32230869  0.37778724]\n",
      " [ 1.          0.94256586  0.78041413  0.88843041  0.73559172  0.60904622]\n",
      " [ 1.          0.9060452   0.88509448  0.8209179   0.8019356   0.78339224]\n",
      " [ 1.          0.55577834  0.35977452  0.30888956  0.19995488  0.1294377 ]\n",
      " [ 1.          0.74621147  0.85815334  0.55683156  0.64036387  0.73642716]\n",
      " [ 1.          0.90000736  0.45876952  0.81001325  0.41289594  0.21046947]\n",
      " [ 1.          0.83633131  0.48933511  0.69945005  0.40924627  0.23944885]\n",
      " [ 1.          0.42334576  0.88099877  0.17922164  0.3729671   0.77615883]\n",
      " [ 1.          0.99486267  0.69561851  0.98975173  0.69204488  0.48388511]\n",
      " [ 1.          0.55435445  0.6567443   0.30730886  0.36406913  0.43131307]\n",
      " [ 1.          0.74904836  0.90553579  0.56107344  0.6782901   0.81999507]]\n",
      "Initial Weights:  [[ 0.75777474]\n",
      " [-0.88286968]\n",
      " [-0.08243992]\n",
      " [-0.68425718]\n",
      " [-0.73102876]\n",
      " [ 0.64880257]] \n",
      "\n",
      "\n",
      "Training Accuracy:  89.898989899  %\n"
     ]
    }
   ],
   "source": [
    "[X,y,weights] = train(X_init, y_init, initial_Weights, num_of_epochs, learning_rate, degree, True,10,'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
