{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Liabraries\n",
    "import pandas as pd # Dataframe and Data Import Library\n",
    "import numpy as np # Linear Algebra Library\n",
    "\n",
    "# Plotting Library\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy.plotting import plot_implicit\n",
    "from sympy.parsing.sympy_parser import parse_expr\n",
    "\n",
    "# Dataset Lib\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]\n",
      " [ 5.4  3.9  1.7  0.4]\n",
      " [ 4.6  3.4  1.4  0.3]\n",
      " [ 5.   3.4  1.5  0.2]\n",
      " [ 4.4  2.9  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.4  3.7  1.5  0.2]\n",
      " [ 4.8  3.4  1.6  0.2]\n",
      " [ 4.8  3.   1.4  0.1]\n",
      " [ 4.3  3.   1.1  0.1]\n",
      " [ 5.8  4.   1.2  0.2]\n",
      " [ 5.7  4.4  1.5  0.4]\n",
      " [ 5.4  3.9  1.3  0.4]\n",
      " [ 5.1  3.5  1.4  0.3]\n",
      " [ 5.7  3.8  1.7  0.3]\n",
      " [ 5.1  3.8  1.5  0.3]\n",
      " [ 5.4  3.4  1.7  0.2]\n",
      " [ 5.1  3.7  1.5  0.4]\n",
      " [ 4.6  3.6  1.   0.2]\n",
      " [ 5.1  3.3  1.7  0.5]\n",
      " [ 4.8  3.4  1.9  0.2]\n",
      " [ 5.   3.   1.6  0.2]\n",
      " [ 5.   3.4  1.6  0.4]\n",
      " [ 5.2  3.5  1.5  0.2]\n",
      " [ 5.2  3.4  1.4  0.2]\n",
      " [ 4.7  3.2  1.6  0.2]\n",
      " [ 4.8  3.1  1.6  0.2]\n",
      " [ 5.4  3.4  1.5  0.4]\n",
      " [ 5.2  4.1  1.5  0.1]\n",
      " [ 5.5  4.2  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.   3.2  1.2  0.2]\n",
      " [ 5.5  3.5  1.3  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 4.4  3.   1.3  0.2]\n",
      " [ 5.1  3.4  1.5  0.2]\n",
      " [ 5.   3.5  1.3  0.3]\n",
      " [ 4.5  2.3  1.3  0.3]\n",
      " [ 4.4  3.2  1.3  0.2]\n",
      " [ 5.   3.5  1.6  0.6]\n",
      " [ 5.1  3.8  1.9  0.4]\n",
      " [ 4.8  3.   1.4  0.3]\n",
      " [ 5.1  3.8  1.6  0.2]\n",
      " [ 4.6  3.2  1.4  0.2]\n",
      " [ 5.3  3.7  1.5  0.2]\n",
      " [ 5.   3.3  1.4  0.2]\n",
      " [ 7.   3.2  4.7  1.4]\n",
      " [ 6.4  3.2  4.5  1.5]\n",
      " [ 6.9  3.1  4.9  1.5]\n",
      " [ 5.5  2.3  4.   1.3]\n",
      " [ 6.5  2.8  4.6  1.5]\n",
      " [ 5.7  2.8  4.5  1.3]\n",
      " [ 6.3  3.3  4.7  1.6]\n",
      " [ 4.9  2.4  3.3  1. ]\n",
      " [ 6.6  2.9  4.6  1.3]\n",
      " [ 5.2  2.7  3.9  1.4]\n",
      " [ 5.   2.   3.5  1. ]\n",
      " [ 5.9  3.   4.2  1.5]\n",
      " [ 6.   2.2  4.   1. ]\n",
      " [ 6.1  2.9  4.7  1.4]\n",
      " [ 5.6  2.9  3.6  1.3]\n",
      " [ 6.7  3.1  4.4  1.4]\n",
      " [ 5.6  3.   4.5  1.5]\n",
      " [ 5.8  2.7  4.1  1. ]\n",
      " [ 6.2  2.2  4.5  1.5]\n",
      " [ 5.6  2.5  3.9  1.1]\n",
      " [ 5.9  3.2  4.8  1.8]\n",
      " [ 6.1  2.8  4.   1.3]\n",
      " [ 6.3  2.5  4.9  1.5]\n",
      " [ 6.1  2.8  4.7  1.2]\n",
      " [ 6.4  2.9  4.3  1.3]\n",
      " [ 6.6  3.   4.4  1.4]\n",
      " [ 6.8  2.8  4.8  1.4]\n",
      " [ 6.7  3.   5.   1.7]\n",
      " [ 6.   2.9  4.5  1.5]\n",
      " [ 5.7  2.6  3.5  1. ]\n",
      " [ 5.5  2.4  3.8  1.1]\n",
      " [ 5.5  2.4  3.7  1. ]\n",
      " [ 5.8  2.7  3.9  1.2]\n",
      " [ 6.   2.7  5.1  1.6]\n",
      " [ 5.4  3.   4.5  1.5]\n",
      " [ 6.   3.4  4.5  1.6]\n",
      " [ 6.7  3.1  4.7  1.5]\n",
      " [ 6.3  2.3  4.4  1.3]\n",
      " [ 5.6  3.   4.1  1.3]\n",
      " [ 5.5  2.5  4.   1.3]\n",
      " [ 5.5  2.6  4.4  1.2]\n",
      " [ 6.1  3.   4.6  1.4]\n",
      " [ 5.8  2.6  4.   1.2]\n",
      " [ 5.   2.3  3.3  1. ]\n",
      " [ 5.6  2.7  4.2  1.3]\n",
      " [ 5.7  3.   4.2  1.2]\n",
      " [ 5.7  2.9  4.2  1.3]\n",
      " [ 6.2  2.9  4.3  1.3]\n",
      " [ 5.1  2.5  3.   1.1]\n",
      " [ 5.7  2.8  4.1  1.3]\n",
      " [ 6.3  3.3  6.   2.5]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 7.1  3.   5.9  2.1]\n",
      " [ 6.3  2.9  5.6  1.8]\n",
      " [ 6.5  3.   5.8  2.2]\n",
      " [ 7.6  3.   6.6  2.1]\n",
      " [ 4.9  2.5  4.5  1.7]\n",
      " [ 7.3  2.9  6.3  1.8]\n",
      " [ 6.7  2.5  5.8  1.8]\n",
      " [ 7.2  3.6  6.1  2.5]\n",
      " [ 6.5  3.2  5.1  2. ]\n",
      " [ 6.4  2.7  5.3  1.9]\n",
      " [ 6.8  3.   5.5  2.1]\n",
      " [ 5.7  2.5  5.   2. ]\n",
      " [ 5.8  2.8  5.1  2.4]\n",
      " [ 6.4  3.2  5.3  2.3]\n",
      " [ 6.5  3.   5.5  1.8]\n",
      " [ 7.7  3.8  6.7  2.2]\n",
      " [ 7.7  2.6  6.9  2.3]\n",
      " [ 6.   2.2  5.   1.5]\n",
      " [ 6.9  3.2  5.7  2.3]\n",
      " [ 5.6  2.8  4.9  2. ]\n",
      " [ 7.7  2.8  6.7  2. ]\n",
      " [ 6.3  2.7  4.9  1.8]\n",
      " [ 6.7  3.3  5.7  2.1]\n",
      " [ 7.2  3.2  6.   1.8]\n",
      " [ 6.2  2.8  4.8  1.8]\n",
      " [ 6.1  3.   4.9  1.8]\n",
      " [ 6.4  2.8  5.6  2.1]\n",
      " [ 7.2  3.   5.8  1.6]\n",
      " [ 7.4  2.8  6.1  1.9]\n",
      " [ 7.9  3.8  6.4  2. ]\n",
      " [ 6.4  2.8  5.6  2.2]\n",
      " [ 6.3  2.8  5.1  1.5]\n",
      " [ 6.1  2.6  5.6  1.4]\n",
      " [ 7.7  3.   6.1  2.3]\n",
      " [ 6.3  3.4  5.6  2.4]\n",
      " [ 6.4  3.1  5.5  1.8]\n",
      " [ 6.   3.   4.8  1.8]\n",
      " [ 6.9  3.1  5.4  2.1]\n",
      " [ 6.7  3.1  5.6  2.4]\n",
      " [ 6.9  3.1  5.1  2.3]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 6.8  3.2  5.9  2.3]\n",
      " [ 6.7  3.3  5.7  2.5]\n",
      " [ 6.7  3.   5.2  2.3]\n",
      " [ 6.3  2.5  5.   1.9]\n",
      " [ 6.5  3.   5.2  2. ]\n",
      " [ 6.2  3.4  5.4  2.3]\n",
      " [ 5.9  3.   5.1  1.8]]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv('ex2data1.txt')\n",
    "iris = datasets.load_iris()\n",
    "# Feature Scaling\n",
    "x1_Normalizer = max(dataset.iloc[:,0])\n",
    "x2_Normalizer = max(dataset.iloc[:,1])\n",
    "dataset.iloc[:,0] = dataset.iloc[:,0]/x1_Normalizer\n",
    "dataset.iloc[:,1] = dataset.iloc[:,1]/x2_Normalizer\n",
    "print(np.array(iris['data']))\n",
    "# X_train = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def featureMap(x1,x2,degree):\n",
    "    x = np.ones([len(x2), 1])\n",
    "    for i in range(1,degree+1):\n",
    "        for j in range(0,i+1):\n",
    "            x = np.concatenate( (x, ((x1**(i-j))*(x2**j)).reshape(len(x1),1)), axis=1 )\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the X and y trainiing sets\n",
    "X_init = dataset.iloc[:,[0,1]]\n",
    "y_init = dataset.iloc[:,2]\n",
    "def create_dataset(X_train, y_train, degree):\n",
    "#     print('X Data: \\n\\n', X_train.head())\n",
    "#     print('\\n\\n')\n",
    "#     print('y Data: \\n\\n', y_train.head())\n",
    "    \n",
    "    # Convert into numpy arrays\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train).reshape(len(y_train),1)\n",
    "#     X_train = featureMap(X_train[:,0],X_train[:,1],degree)\n",
    "\n",
    "    m = len(y_train) # Number of Examples\n",
    "    print('Training Set: ', X_train)\n",
    "    \n",
    "    return [X_train, y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the Data\n",
    "def plot(X,y,w):\n",
    "    # Seperate Classes\n",
    "    X_postive1 = X[:,1].reshape(99,1)[y == 1].reshape(60,1)\n",
    "    X_postive2 = X[:,2].reshape(99,1)[y == 1].reshape(60,1)\n",
    "    X_postive = np.concatenate( (X_postive1, X_postive2 ), axis =1)\n",
    "\n",
    "    X_negative1 = X[:,1].reshape(99,1)[y == 0].reshape(39,1)\n",
    "    X_negative2 = X[:,2].reshape(99,1)[y == 0].reshape(39,1)\n",
    "    X_negative = np.concatenate( (X_negative1, X_negative2 ), axis =1)\n",
    "    plt.figure\n",
    "    plt.scatter(X_postive[:,0], X_postive[:,1], marker = 'x', color = 'r', EdgeColor = 'r')\n",
    "    plt.scatter(X_negative[:,0], X_negative[:,1], marker = 'o', color='y', EdgeColor = 'b')\n",
    "    plt.xlabel('Exam 1 Scores')\n",
    "    plt.ylabel('Exam 1 Scores')\n",
    "#     if w.any():\n",
    "#         plot_x = [min(X_train[:,1])-0.1,  max(X_train[:,1])+0.1]\n",
    "#         plot_y = (-1./weights[2])*(weights[1]*plot_x + weights[0]);\n",
    "#         plt.plot(plot_x, plot_y)\n",
    "    plt.show()\n",
    "    \n",
    "def ezplot(s):\n",
    "    #Parse doesn't parse = sign so split\n",
    "    lhs, rhs = s.replace(\"^\",\"**\").split(\"=\")\n",
    "    eqn_lhs = parse_expr(lhs)\n",
    "    eqn_rhs = parse_expr(rhs)\n",
    "\n",
    "    plot_implicit(eqn_lhs-eqn_rhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost function with Derivation\n",
    "def sigmoid(deriv, x):\n",
    "    if deriv == True:\n",
    "        return sigmoid(False, x)*(1-sigmoid(False, x))\n",
    "    return 1.0/(1 + np.exp(-x))\n",
    "\n",
    "def perceptron_func(deriv,z):\n",
    "    return np.array(z>=0, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impliment the Cross Entropy Log Loss Function \n",
    "def costFunction(X,y,initW,current_epoch,stochastic=False,mini_batch_size=10, cost_func = 'sigmoid'):\n",
    "    m = len(y)\n",
    "    J = 0\n",
    "    derivatives = np.zeros([X.shape[1],1])\n",
    "    g = np.dot(X,initW)  \n",
    "    \n",
    "    if (cost_func == 'sigmoid'):\n",
    "        hypothesis = sigmoid(False,g)\n",
    "        J = sum (-1*np.dot(y.reshape(m), np.log(hypothesis)) - ( np.dot((1-y.reshape(m)),np.log(1-hypothesis)) )) / m \n",
    "        if not current_epoch%1000:\n",
    "            print('Error at iteration ', current_epoch, ': ', J)\n",
    "            print('Still Learning - Trying to get better...\\n\\n')\n",
    "    elif (cost_func == 'perceptron'):\n",
    "        hypothesis = perceptron_func(False,g)\n",
    "        \n",
    "#     for i in range(0,len(y)):\n",
    "#         newCost = ( -1*y[i]*math.log(hypothesis[i]) ) - ( (1-y[i])*math.log(1-hypothesis[i]) )\n",
    "#         J = J + newCost\n",
    "        \n",
    "#     J = J/m\n",
    "#     print( (hypothesis-y).reshape() )\n",
    "    \n",
    "#     derivatives = np.sum( np.dot( (hypothesis - y).reshape(y.shape[0]), X ).reshape(2,1), axis = 1).reshape(X_train.shape[1],1) / m\n",
    "\n",
    "    if stochastic:\n",
    "        batch_size = 10\n",
    "    else:\n",
    "        batch_size = m\n",
    "\n",
    "    for i in range(0,len(derivatives)):\n",
    "        for j in range(0,batch_size):\n",
    "            diffCost = ( hypothesis[j] - y[j] ) * X[j,i]\n",
    "            derivatives[i] = derivatives[i] + diffCost\n",
    "        derivatives[i] = derivatives[i]/m\n",
    "\n",
    "#     print('Derivatives: ', derivatives)\n",
    "    return derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x,weights,cost_func):\n",
    "    X = x # Create a Copy\n",
    "    predictions = []\n",
    "    for i in range(0,len(X)):\n",
    "        x = featureMap(np.array([X[i,0]]),np.array([X[i,1]]),degree)\n",
    "        g = np.dot(x,weights)  \n",
    "        if (cost_func == 'sigmoid'):\n",
    "            prediction = np.round(sigmoid(False,g))[0,0]\n",
    "        elif (cost_func == 'perceptron'):\n",
    "            prediction = np.round(perceptron_func(False,g))[0,0]\n",
    "        predictions.append(prediction)\n",
    "        #print('The Predicted Class for the input is: ', prediction)\n",
    "    return np.array(predictions).reshape(len(predictions),1)\n",
    "\n",
    "def accuracy(X,y,weights,cost_func):\n",
    "    test_vals = np.concatenate((X[:,1].reshape(len(X[:,1]),1), X[:,2].reshape(len(X[:,2]),1)), axis = 1)\n",
    "    predictions = predict(test_vals,weights,cost_func)\n",
    "    print('Training Accuracy: ', np.mean(predictions == y) * 100, ' %')\n",
    "    \n",
    "def train(X, y, initW, num_of_epochs, learning_rate, degree, stochastic=False,mini_batch_size=10, cost_func='sigmoid'):\n",
    "    [X, y] = create_dataset(X, y, degree)\n",
    "    if not len(initW):\n",
    "        # Randomly Initialize the weights\n",
    "        initW = np.random.randn(X.shape[1],1)\n",
    "    weights = initW\n",
    "    print('Initial Weights: ', initW, '\\n\\n')\n",
    "    for i in range(1,num_of_epochs):\n",
    "        deriv = costFunction(X,y,weights,i,stochastic,mini_batch_size, cost_func)\n",
    "        weights = weights - learning_rate*deriv \n",
    "#         print('Weights: ',weights)\n",
    "#     plot(weights)\n",
    "    accuracy(X,y,weights,cost_func)\n",
    "    return [X,y,weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "degree = 1 # Needs to be set explicitly globally outside the train function\n",
    "num_of_epochs = 1000\n",
    "learning_rate = 0.5\n",
    "initial_Weights = np.random.randn(3,1)\n",
    "\n",
    "X_init = iris['data']\n",
    "y_init = np.zeros([150,1])\n",
    "y_init[0:49] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:  [[ 1.          0.30338937  0.44396933]\n",
      " [ 1.          0.35909224  0.73735829]\n",
      " [ 1.          0.60286378  0.87295484]\n",
      " [ 1.          0.79169019  0.76205933]\n",
      " [ 1.          0.45161019  0.56960345]\n",
      " [ 1.          0.61212036  0.97615026]\n",
      " [ 1.          0.75154118  0.47086355]\n",
      " [ 1.          0.76230011  0.88420217]\n",
      " [ 1.          0.84578415  0.44031194]\n",
      " [ 1.          0.96026858  0.38662381]\n",
      " [ 1.          0.75143011  0.30953209]\n",
      " [ 1.          0.82448983  0.77356529]\n",
      " [ 1.          0.694842    0.98836098]\n",
      " [ 1.          0.39606519  0.76906286]\n",
      " [ 1.          0.54064119  0.90227429]\n",
      " [ 1.          0.69189248  0.53343553]\n",
      " [ 1.          0.68064022  0.4721234 ]\n",
      " [ 1.          0.70783358  0.93989752]\n",
      " [ 1.          0.77111525  0.48119991]\n",
      " [ 1.          0.67488203  0.43328293]\n",
      " [ 1.          0.89831414  0.66551777]\n",
      " [ 1.          0.5062193   0.49414474]\n",
      " [ 1.          0.34271056  0.44715061]\n",
      " [ 1.          0.78058463  0.69761054]\n",
      " [ 1.          0.62378393  0.70754382]\n",
      " [ 1.          0.8032846   0.45334161]\n",
      " [ 1.          0.93274954  0.39244353]\n",
      " [ 1.          0.61936825  0.50830783]\n",
      " [ 1.          0.38852686  0.65738901]\n",
      " [ 1.          0.61485131  0.7364044 ]\n",
      " [ 1.          0.8555179   0.57704369]\n",
      " [ 1.          0.52197834  0.63849483]\n",
      " [ 1.          0.52135151  0.70226819]\n",
      " [ 1.          0.40306278  0.71981546]\n",
      " [ 1.          0.54729318  0.52810948]\n",
      " [ 1.          0.33973984  1.        ]\n",
      " [ 1.          0.64287655  0.81833238]\n",
      " [ 1.          0.74918219  0.42048804]\n",
      " [ 1.          0.34242586  0.76098058]\n",
      " [ 1.          0.84047074  0.56951924]\n",
      " [ 1.          0.51636609  0.47392088]\n",
      " [ 1.          0.94606225  0.66318697]\n",
      " [ 1.          0.8251079   0.41082722]\n",
      " [ 1.          0.51135778  0.4634668 ]\n",
      " [ 1.          0.62329972  0.52656305]\n",
      " [ 1.          0.77326146  0.71263884]\n",
      " [ 1.          0.97940196  0.87719548]\n",
      " [ 1.          0.62180102  0.97875368]\n",
      " [ 1.          0.91722868  0.89710528]\n",
      " [ 1.          0.80082674  0.75011169]\n",
      " [ 1.          0.99443712  0.6169655 ]\n",
      " [ 1.          0.90702852  0.4388677 ]\n",
      " [ 1.          0.34584048  0.6108697 ]\n",
      " [ 1.          0.5037321   0.5037405 ]\n",
      " [ 1.          0.49672184  0.60492862]\n",
      " [ 1.          0.97814013  0.69648999]\n",
      " [ 1.          0.32633376  0.9669171 ]\n",
      " [ 1.          0.74376725  0.7062301 ]\n",
      " [ 1.          0.71920267  0.79350673]\n",
      " [ 1.          0.75525623  0.86740595]\n",
      " [ 1.          0.3534696   0.4755819 ]\n",
      " [ 1.          0.56350821  0.39710424]\n",
      " [ 1.          0.30110656  0.50160066]\n",
      " [ 1.          0.44745287  0.67209938]\n",
      " [ 1.          0.66675671  0.41561983]\n",
      " [ 1.          0.40527316  0.98650493]\n",
      " [ 1.          0.49157183  0.52476492]\n",
      " [ 1.          0.80418007  0.93169401]\n",
      " [ 1.          0.66861816  0.61688826]\n",
      " [ 1.          0.3277926   0.43802387]\n",
      " [ 1.          0.64149749  0.78923974]\n",
      " [ 1.          0.72471248  0.97327948]\n",
      " [ 1.          0.60562139  0.73930834]\n",
      " [ 1.          0.58942421  0.76725884]\n",
      " [ 1.          1.          0.73196789]\n",
      " [ 1.          0.47345771  0.8948758 ]\n",
      " [ 1.          0.50545169  0.76676739]\n",
      " [ 1.          0.60559805  0.4299449 ]\n",
      " [ 1.          0.82368452  0.43208377]\n",
      " [ 1.          0.89067219  0.7060199 ]\n",
      " [ 1.          0.94998038  0.46216818]\n",
      " [ 1.          0.67435342  0.67350797]\n",
      " [ 1.          0.57337408  0.60194823]\n",
      " [ 1.          0.8050534   0.9200027 ]\n",
      " [ 1.          0.68586588  0.86573071]\n",
      " [ 1.          0.42148009  0.7974637 ]\n",
      " [ 1.          0.75607855  0.91458537]\n",
      " [ 1.          0.78771023  0.97752583]\n",
      " [ 1.          0.52438272  0.614644  ]\n",
      " [ 1.          0.94256586  0.78041413]\n",
      " [ 1.          0.9060452   0.88509448]\n",
      " [ 1.          0.55577834  0.35977452]\n",
      " [ 1.          0.74621147  0.85815334]\n",
      " [ 1.          0.90000736  0.45876952]\n",
      " [ 1.          0.83633131  0.48933511]\n",
      " [ 1.          0.42334576  0.88099877]\n",
      " [ 1.          0.99486267  0.69561851]\n",
      " [ 1.          0.55435445  0.6567443 ]\n",
      " [ 1.          0.74904836  0.90553579]]\n",
      "Initial Weights:  [[ 1.8559139 ]\n",
      " [-2.06681104]\n",
      " [ 1.0327797 ]] \n",
      "\n",
      "\n",
      "Training Accuracy:  91.9191919192  %\n"
     ]
    }
   ],
   "source": [
    "[X,y,weights] = train(X_init, y_init, initial_Weights, num_of_epochs, learning_rate, degree, False,10,'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_init = np.zeros([150,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
